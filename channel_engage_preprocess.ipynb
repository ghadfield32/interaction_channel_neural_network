{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   acct_ref_nb  acct_balance  tot_delq_amt  min_delq_amt  probability_score  \\\n",
      "0       125001           250           100            25              0.534   \n",
      "1       125001           250           100            25              0.534   \n",
      "2       125001           250           100            25              0.534   \n",
      "3       125001           250           100            25              0.534   \n",
      "4       125001           250           100            25              0.534   \n",
      "\n",
      "  src_data_dt  cycle_dt state  zip_code   dlq_30   dlq_60   dlq_90  \\\n",
      "0  2022-01-31  1/5/2022    KY     40288  current  current  current   \n",
      "1  2022-01-31  1/5/2022    KY     40288  current  current  current   \n",
      "2  2022-01-31  1/5/2022    KY     40288  current  current  current   \n",
      "3  2022-01-31  1/5/2022    KY     40288  current  current  current   \n",
      "4  2022-01-31  1/5/2022    KY     40288  current  current  current   \n",
      "\n",
      "  exclusion_type       roll_30       roll_60       roll_90 segment  bucket  \\\n",
      "0       elgiible  roll forward  roll forward  roll forward    cure       1   \n",
      "1       elgiible  roll forward  roll forward  roll forward    cure       1   \n",
      "2       elgiible  roll forward  roll forward  roll forward    cure       1   \n",
      "3       elgiible  roll forward  roll forward  roll forward    cure       1   \n",
      "4       elgiible  roll forward  roll forward  roll forward    cure       1   \n",
      "\n",
      "       month  \n",
      "0 2022-01-01  \n",
      "1 2022-01-01  \n",
      "2 2022-01-01  \n",
      "3 2022-01-01  \n",
      "4 2022-01-01  \n",
      "\n",
      "     acct_ref_nb        date    channel      month\n",
      "0         125001  2022-01-31      EMAIL 2022-01-01\n",
      "1         125001  2022-01-31       TEXT 2022-01-01\n",
      "4         125001  2022-01-31   OUTBOUND 2022-01-01\n",
      "6         125001  2022-01-31       PYMT 2022-01-01\n",
      "7         125001  2022-01-31    PROMISE 2022-01-01\n",
      "8         125001  2022-01-31  PYMT_PROG 2022-01-01\n",
      "15        125001  2022-02-01       PYMT 2022-02-01\n",
      "16        125001  2022-02-01      EMAIL 2022-02-01\n",
      "17        125001  2022-02-01       TEXT 2022-02-01\n",
      "20        125001  2022-02-01   OUTBOUND 2022-02-01\n",
      "23        125001  2022-02-01    PROMISE 2022-02-01\n",
      "24        125001  2022-02-01  PYMT_PROG 2022-02-01\n",
      "29        125001  2022-02-02       PYMT 2022-02-01\n",
      "30        125001  2022-02-02    PROMISE 2022-02-01\n",
      "31        125001  2022-02-03       PYMT 2022-02-01\n",
      "32        125001  2022-02-04  PYMT_PROG 2022-02-01\n",
      "33        125001  2022-02-05    PROMISE 2022-02-01\n",
      "34        125001  2022-02-03      EMAIL 2022-02-01\n",
      "35        125001  2022-02-04   OUTBOUND 2022-02-01\n",
      "36        125001  2022-02-05       TEXT 2022-02-01\n",
      "37        125001  2022-02-02      EMAIL 2022-02-01\n",
      "39        125001  2022-02-04      EMAIL 2022-02-01\n",
      "40        125001  2022-02-05      EMAIL 2022-02-01\n",
      "41        125001  2022-02-06      EMAIL 2022-02-01\n",
      "42        125001  2022-02-07      EMAIL 2022-02-01\n",
      "43        125001  2022-02-08      EMAIL 2022-02-01\n",
      "44        125001  2022-02-09      EMAIL 2022-02-01\n",
      "45        125001  2022-02-10      EMAIL 2022-02-01\n",
      "46        125001  2022-02-11      EMAIL 2022-02-01\n",
      "47        125001  2022-02-12      EMAIL 2022-02-01\n",
      "48        125001  2022-02-13      EMAIL 2022-02-01\n",
      "49        125001  2022-02-14      EMAIL 2022-02-01\n",
      "50        125001  2022-02-15      EMAIL 2022-02-01\n",
      "51        125001  2022-02-16      EMAIL 2022-02-01\n",
      "53        125001  2022-02-06       TEXT 2022-02-01\n",
      "54        125001  2022-02-07       TEXT 2022-02-01\n",
      "55        125001  2022-02-08       TEXT 2022-02-01\n",
      "56        125001  2022-02-09       TEXT 2022-02-01\n",
      "57        125001  2022-02-10       TEXT 2022-02-01\n",
      "58        125001  2022-02-11       TEXT 2022-02-01\n",
      "59        125001  2022-02-12       TEXT 2022-02-01\n",
      "60        125001  2022-02-13       TEXT 2022-02-01\n",
      "61        125001  2022-02-05   OUTBOUND 2022-02-01\n",
      "62        125001  2022-02-06   OUTBOUND 2022-02-01\n",
      "63        125001  2022-02-07   OUTBOUND 2022-02-01\n",
      "64        125001  2022-02-08   OUTBOUND 2022-02-01\n",
      "65        125001  2022-02-09   OUTBOUND 2022-02-01\n",
      "66        125001  2022-02-10   OUTBOUND 2022-02-01\n",
      "67        125001  2022-02-11   OUTBOUND 2022-02-01\n",
      "68        125001  2022-02-12   OUTBOUND 2022-02-01\n",
      "69        125001  2022-02-13   OUTBOUND 2022-02-01\n",
      "70        125002  2022-01-31      EMAIL 2022-01-01\n",
      "71        125002  2022-01-31       TEXT 2022-01-01\n",
      "74        125002  2022-01-31   OUTBOUND 2022-01-01\n",
      "76        125002  2022-01-31       PYMT 2022-01-01\n",
      "77        125002  2022-01-31    PROMISE 2022-01-01\n",
      "78        125002  2022-01-31  PYMT_PROG 2022-01-01\n",
      "85        125002  2022-02-01       PYMT 2022-02-01\n",
      "86        125002  2022-02-01      EMAIL 2022-02-01\n",
      "87        125002  2022-02-01       TEXT 2022-02-01\n",
      "90        125002  2022-02-01   OUTBOUND 2022-02-01\n",
      "93        125002  2022-02-01    PROMISE 2022-02-01\n",
      "94        125002  2022-02-01  PYMT_PROG 2022-02-01\n",
      "99        125002  2022-02-02       PYMT 2022-02-01\n",
      "100       125002  2022-02-02    PROMISE 2022-02-01\n",
      "101       125002  2022-02-03       PYMT 2022-02-01\n",
      "102       125002  2022-02-04  PYMT_PROG 2022-02-01\n",
      "103       125002  2022-02-05    PROMISE 2022-02-01\n",
      "104       125002  2022-02-03      EMAIL 2022-02-01\n",
      "105       125002  2022-02-04   OUTBOUND 2022-02-01\n",
      "106       125002  2022-02-05       TEXT 2022-02-01\n",
      "107       125002  2022-02-02      EMAIL 2022-02-01\n",
      "109       125002  2022-02-04      EMAIL 2022-02-01\n",
      "110       125002  2022-02-05      EMAIL 2022-02-01\n",
      "111       125002  2022-02-06      EMAIL 2022-02-01\n",
      "112       125002  2022-02-07      EMAIL 2022-02-01\n",
      "113       125002  2022-02-08      EMAIL 2022-02-01\n",
      "114       125002  2022-02-09      EMAIL 2022-02-01\n",
      "115       125002  2022-02-10      EMAIL 2022-02-01\n",
      "116       125002  2022-02-11      EMAIL 2022-02-01\n",
      "117       125002  2022-02-12      EMAIL 2022-02-01\n",
      "118       125002  2022-02-13      EMAIL 2022-02-01\n",
      "119       125002  2022-02-14      EMAIL 2022-02-01\n",
      "120       125002  2022-02-15      EMAIL 2022-02-01\n",
      "121       125002  2022-02-16      EMAIL 2022-02-01\n",
      "123       125002  2022-02-06       TEXT 2022-02-01\n",
      "124       125002  2022-02-07       TEXT 2022-02-01\n",
      "125       125002  2022-02-08       TEXT 2022-02-01\n",
      "126       125002  2022-02-09       TEXT 2022-02-01\n",
      "127       125002  2022-02-10       TEXT 2022-02-01\n",
      "128       125002  2022-02-11       TEXT 2022-02-01\n",
      "129       125002  2022-02-12       TEXT 2022-02-01\n",
      "130       125002  2022-02-13       TEXT 2022-02-01\n",
      "131       125002  2022-02-05   OUTBOUND 2022-02-01\n",
      "132       125002  2022-02-06   OUTBOUND 2022-02-01\n",
      "133       125002  2022-02-07   OUTBOUND 2022-02-01\n",
      "134       125002  2022-02-08   OUTBOUND 2022-02-01\n",
      "135       125002  2022-02-09   OUTBOUND 2022-02-01\n",
      "136       125002  2022-02-10   OUTBOUND 2022-02-01\n",
      "137       125002  2022-02-11   OUTBOUND 2022-02-01\n",
      "138       125002  2022-02-12   OUTBOUND 2022-02-01\n",
      "139       125002  2022-02-13   OUTBOUND 2022-02-01\n",
      "140       125003  2022-01-31      EMAIL 2022-01-01\n",
      "141       125003  2022-01-31       TEXT 2022-01-01\n",
      "144       125003  2022-01-31   OUTBOUND 2022-01-01\n",
      "146       125003  2022-01-31       PYMT 2022-01-01\n",
      "147       125003  2022-01-31    PROMISE 2022-01-01\n",
      "148       125003  2022-01-31  PYMT_PROG 2022-01-01\n",
      "155       125003  2022-02-01       PYMT 2022-02-01\n",
      "156       125003  2022-02-01      EMAIL 2022-02-01\n",
      "157       125003  2022-02-01       TEXT 2022-02-01\n",
      "160       125003  2022-02-01   OUTBOUND 2022-02-01\n",
      "163       125003  2022-02-01    PROMISE 2022-02-01\n",
      "164       125003  2022-02-01  PYMT_PROG 2022-02-01\n",
      "169       125003  2022-02-02       PYMT 2022-02-01\n",
      "170       125003  2022-02-02    PROMISE 2022-02-01\n",
      "171       125003  2022-02-03       PYMT 2022-02-01\n",
      "172       125003  2022-02-04  PYMT_PROG 2022-02-01\n",
      "173       125003  2022-02-05    PROMISE 2022-02-01\n",
      "174       125003  2022-02-03      EMAIL 2022-02-01\n",
      "175       125003  2022-02-04   OUTBOUND 2022-02-01\n",
      "176       125003  2022-02-05       TEXT 2022-02-01\n",
      "177       125003  2022-02-02      EMAIL 2022-02-01\n",
      "179       125003  2022-02-04      EMAIL 2022-02-01\n",
      "180       125003  2022-02-05      EMAIL 2022-02-01\n",
      "181       125003  2022-02-06      EMAIL 2022-02-01\n",
      "182       125003  2022-02-07      EMAIL 2022-02-01\n",
      "183       125003  2022-02-08      EMAIL 2022-02-01\n",
      "184       125003  2022-02-09      EMAIL 2022-02-01\n",
      "185       125003  2022-02-10      EMAIL 2022-02-01\n",
      "186       125003  2022-02-11      EMAIL 2022-02-01\n",
      "187       125003  2022-02-12      EMAIL 2022-02-01\n",
      "188       125003  2022-02-13      EMAIL 2022-02-01\n",
      "189       125003  2022-02-14      EMAIL 2022-02-01\n",
      "190       125003  2022-02-15      EMAIL 2022-02-01\n",
      "191       125003  2022-02-16      EMAIL 2022-02-01\n",
      "193       125003  2022-02-06       TEXT 2022-02-01\n",
      "194       125003  2022-02-07       TEXT 2022-02-01\n",
      "195       125003  2022-02-08       TEXT 2022-02-01\n",
      "196       125003  2022-02-09       TEXT 2022-02-01\n",
      "197       125003  2022-02-10       TEXT 2022-02-01\n",
      "198       125003  2022-02-11       TEXT 2022-02-01\n",
      "199       125003  2022-02-12       TEXT 2022-02-01\n",
      "200       125003  2022-02-13       TEXT 2022-02-01\n",
      "201       125003  2022-02-05   OUTBOUND 2022-02-01\n",
      "202       125003  2022-02-06   OUTBOUND 2022-02-01\n",
      "203       125003  2022-02-07   OUTBOUND 2022-02-01\n",
      "204       125003  2022-02-08   OUTBOUND 2022-02-01\n",
      "205       125003  2022-02-09   OUTBOUND 2022-02-01\n",
      "206       125003  2022-02-10   OUTBOUND 2022-02-01\n",
      "207       125003  2022-02-11   OUTBOUND 2022-02-01\n",
      "208       125003  2022-02-12   OUTBOUND 2022-02-01\n",
      "209       125003  2022-02-13   OUTBOUND 2022-02-01\n",
      "(105, 19) (153, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from itertools import chain, combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "jan_dlq2 = pd.read_csv(r\"C:\\Users\\ghadf\\OneDrive\\Desktop\\Data Analytics\\Python\\DL\\channel_engage\\data\\jan_dlq2.csv\")\n",
    "events_df = pd.read_csv(r\"C:\\Users\\ghadf\\OneDrive\\Desktop\\Data Analytics\\Python\\DL\\channel_engage\\data\\events_df.csv\")\n",
    "\n",
    "#work\n",
    "#filtered to speed up creation time\n",
    "#filtered_accounts = ['1581216589''1250636031']\n",
    "#jan_dlq2 = jan_dlq2[jan_dlq2['acct_ref_nb'].isin(filtered_accounts)]\n",
    "#events_df = events_df[events_df['acct_ref_nb'].isin(filtered_accounts)]\n",
    "\n",
    "#update emailday2 to email and ob_attempts to outbound in events_df\n",
    "#work\n",
    "#events_df['channel'] = events_df['channel'].replace(['EMAILDAY2'], 'EMAIL')\n",
    "#events_df['channel'] = events_df['channel'].replace(['OB_ATTEMPTS'], 'OUTBOUND')\n",
    "#drop acc_balance segment probability_score and bucket from events df because it's already in jan_dlq2\n",
    "#work\n",
    "#events_df = events_df.drop(['acc_balance_segment', 'probability_score', 'bucket'])\n",
    "#drop duplicates in events_df\n",
    "events_df = events_df.drop_duplicates()\n",
    "#check on E_LETTER or E_LETTER\n",
    "interaction_channels = ['EMAIL', 'TEXT', 'LETTER', 'E_LETTER', 'OUTBOUND'] #Maybe take out letter and E_LETTER because timing is off unless it's 15 day letter\n",
    "#For Updating the interaction channels list if we want to retrain on a different set of interaction channels\n",
    "drop_channels = ['LETTER', 'E_LETTER']\n",
    "#drop the columns in events_df that are in the channel column in drop_channels\n",
    "events_df = events_df[~events_df['channel'].isin(drop_channels)]\n",
    "#new list of interaction_channels\n",
    "interaction_channels = [x for x in interaction_channels if x not in drop_channels]\n",
    "\n",
    "target_channels = ['INBOUND', 'PYMT', 'LOGIN', 'PROMISE', 'PYMT_PROG']\n",
    "#For Updating the target channels list if we want to retrain on a different set of target channels\n",
    "drop_channels = ['INBOUND','LOGIN'] \n",
    "#drop the columns in events_df that are in the channel column in drop_channels\n",
    "events_df = events_df[~events_df['channel'].isin(drop_channels)]\n",
    "#new list of target_channels\n",
    "target_channels = [x for x in target_channels if x not in drop_channels]\n",
    "\n",
    "#fix the date formattings so it's usable\n",
    "#work\n",
    "#jan_dlq2['src_data_dt'] = pd.to_datetime(jan_dlq2['src_data_dt'], format=\"%d%b%y:%H:%M:%S\").dt.date\n",
    "#events_df['date'] = pd.to_datetime(events_df[\"date\"], format='%d%b%Y').dt.date\n",
    "#not work\n",
    "jan_dlq2['src_data_dt'] = pd.to_datetime(jan_dlq2['src_data_dt'], format=\"%d%b%y:%H:%M:%S\").dt.date\n",
    "events_df['date'] = pd.to_datetime(events_df[\"date\"], format='%d-%b-%y').dt.date\n",
    "\n",
    "\n",
    "#get month from cycle_dt and date so we can merge on month\n",
    "#work\n",
    "#jan_dlq2['month'] = pd.to_datetime(jan_dlq2[\"cycle_dt\"], format='%m/%d/%Y').dt.to_period('M').dt.to_timestamp()\n",
    "#events_df['month'] = pd.to_datetime(events_df[\"date\"], format='%d%b%Y').dt.to_period('M').dt.to_timestamp()\n",
    "#not work\n",
    "jan_dlq2['month'] = pd.to_datetime(jan_dlq2[\"cycle_dt\"], format='%m/%d/%Y').dt.to_period('M').dt.to_timestamp()\n",
    "events_df['month'] = pd.to_datetime(events_df[\"date\"], format='%Y-%m-%d').dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "#review the data\n",
    "print(jan_dlq2.head(), events_df, sep='\\n\\n')\n",
    "print(jan_dlq2.shape, events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   acct_ref_nb        date      month  EMAIL  OUTBOUND  TEXT target_channel\n",
      "0       125001  2022-01-31 2022-01-01      1         1     1           PYMT\n",
      "1       125001  2022-02-01 2022-02-01      1         1     1           PYMT\n",
      "2       125001  2022-02-02 2022-02-01      1         0     0           PYMT\n",
      "3       125001  2022-02-03 2022-02-01      1         0     0           PYMT\n",
      "4       125001  2022-02-04 2022-02-01      1         1     0           PYMT\n",
      "\n",
      "(51, 7)\n"
     ]
    }
   ],
   "source": [
    "#preprocess by concatenating events_df into a row per date, acct_ref_nb, and target_channel type.\n",
    "#^^ this allows the volume to stay correct and us to be able to assess target channel's by their combinations\n",
    "# One-hot encode the 'channel' column\n",
    "df_concatenate = pd.get_dummies(events_df, columns=['channel'], prefix='', prefix_sep='')\n",
    "\n",
    "# Aggregate interaction channels by 'acct_ref_nb' and 'date'\n",
    "df_agg_concatenate = df_concatenate.drop(columns=target_channels).groupby(['acct_ref_nb', 'date', 'month']).sum().reset_index()\n",
    "\n",
    "# Aggregate target channels for each 'acct_ref_nb' and 'date'\n",
    "df_targets_concatenate = df_concatenate.groupby(['acct_ref_nb', 'date', 'month'])[target_channels].sum().reset_index()\n",
    "df_targets_concatenate['target_channel'] = df_targets_concatenate[target_channels].apply(lambda x: '_'.join(x.index[x > 0]), axis=1)\n",
    "\n",
    "# Merge the aggregated target channels back into the feature dataframe\n",
    "df_agg_concatenate = df_agg_concatenate.merge(df_targets_concatenate[['acct_ref_nb', 'date', 'month', 'target_channel']], on=['acct_ref_nb', 'date', 'month'], how='left')\n",
    "\n",
    "#replace blanks with 'NO_PYMT'\n",
    "df_agg_concatenate['target_channel'] = df_agg_concatenate['target_channel'].replace([''], 'NO_PYMT')\n",
    "\n",
    "# Replace non-blanks with 'PYMT' except for 'NO_PYMT'\n",
    "mask = (df_agg_concatenate['target_channel'] != '') & (df_agg_concatenate['target_channel'] != 'NO_PYMT')\n",
    "df_agg_concatenate.loc[mask, 'target_channel'] = 'PYMT'\n",
    "\n",
    "print(df_agg_concatenate.head(), df_agg_concatenate.shape, sep='\\n\\n')\n",
    "\n",
    "#export to csv\n",
    "df_agg_concatenate.to_csv(r\"C:\\Users\\ghadf\\OneDrive\\Desktop\\Data Analytics\\Python\\DL\\channel_engage\\data\\df_agg_concatenate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   acct_ref_nb        date      month  EMAIL  OUTBOUND  TEXT target_channel\n",
      "0       125001  2022-01-31 2022-01-01      1         1     1           PYMT\n",
      "1       125001  2022-02-01 2022-02-01      1         1     1           PYMT\n",
      "2       125001  2022-02-02 2022-02-01      1         0     0           PYMT\n",
      "3       125001  2022-02-03 2022-02-01      1         0     0           PYMT\n",
      "4       125001  2022-02-04 2022-02-01      1         1     0           PYMT\n",
      "NO_PYMT    33\n",
      "PYMT       18\n",
      "Name: target_channel, dtype: int64\n",
      "Missing keys in df_agg_concatenate: 4\n",
      "Missing keys in jan_dlq_no_dupes: 5\n",
      "Duplicates in df_agg_concatenate: 45\n",
      "Duplicates in jan_dlq_no_dupes: 13\n",
      "NaN values in df_agg_concatenate: 0\n",
      "NaN values in jan_dlq_no_dupes: 0\n"
     ]
    }
   ],
   "source": [
    "#change everything in the target_channel not named 'NO_PYMT' to 'PYMT'\n",
    "\n",
    "print(df_agg_concatenate.head())\n",
    "#print target_channel value counts\n",
    "print(df_agg_concatenate['target_channel'].value_counts())\n",
    "\n",
    "#get unique jan_dlq2 rows to get categorical variables for the model\n",
    "jan_dlq_no_dupes = jan_dlq2.drop_duplicates()\n",
    "\n",
    "unique_keys_df1 = df_agg_concatenate[['acct_ref_nb', 'month']].drop_duplicates()\n",
    "unique_keys_df2 = jan_dlq_no_dupes[['acct_ref_nb', 'month']].drop_duplicates()\n",
    "\n",
    "merged_keys = pd.merge(unique_keys_df1, unique_keys_df2, on=['acct_ref_nb', 'month'], how='inner')\n",
    "missing_keys_df1 = unique_keys_df1[~unique_keys_df1.isin(merged_keys)].dropna()\n",
    "missing_keys_df2 = unique_keys_df2[~unique_keys_df2.isin(merged_keys)].dropna()\n",
    "\n",
    "print(\"Missing keys in df_agg_concatenate:\", len(missing_keys_df1))\n",
    "print(\"Missing keys in jan_dlq_no_dupes:\", len(missing_keys_df2))\n",
    "\n",
    "duplicates_df1 = df_agg_concatenate[df_agg_concatenate.duplicated(subset=['acct_ref_nb', 'month'])]\n",
    "duplicates_df2 = jan_dlq_no_dupes[jan_dlq_no_dupes.duplicated(subset=['acct_ref_nb', 'month'])]\n",
    "\n",
    "print(\"Duplicates in df_agg_concatenate:\", len(duplicates_df1))\n",
    "print(\"Duplicates in jan_dlq_no_dupes:\", len(duplicates_df2))\n",
    "\n",
    "nan_values_df1 = df_agg_concatenate[df_agg_concatenate[['acct_ref_nb', 'month']].isnull().any(axis=1)]\n",
    "nan_values_df2 = jan_dlq_no_dupes[jan_dlq_no_dupes[['acct_ref_nb', 'month']].isnull().any(axis=1)]\n",
    "\n",
    "print(\"NaN values in df_agg_concatenate:\", len(nan_values_df1))\n",
    "print(\"NaN values in jan_dlq_no_dupes:\", len(nan_values_df2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   acct_ref_nb        date      month  EMAIL  OUTBOUND  TEXT target_channel  \\\n",
      "0       125001  2022-01-31 2022-01-01      1         1     1           PYMT   \n",
      "1       125001  2022-02-01 2022-02-01      1         1     1           PYMT   \n",
      "2       125001  2022-02-01 2022-02-01      1         1     1           PYMT   \n",
      "3       125001  2022-02-01 2022-02-01      1         1     1           PYMT   \n",
      "4       125001  2022-02-01 2022-02-01      1         1     1           PYMT   \n",
      "\n",
      "   acct_balance  tot_delq_amt  min_delq_amt  probability_score src_data_dt  \\\n",
      "0           250           100            25              0.534  2022-01-31   \n",
      "1           500           250            50              0.734  2022-02-01   \n",
      "2           500           250            50              0.434  2022-02-02   \n",
      "3           500           250            50              0.334  2022-02-02   \n",
      "4           500           250            50              0.120  2022-02-03   \n",
      "\n",
      "   cycle_dt state  zip_code   dlq_30   dlq_60   dlq_90 exclusion_type  \\\n",
      "0  1/5/2022    KY     40288  current  current  current       elgiible   \n",
      "1  2/6/2022    KY     40288  current  current  current       elgiible   \n",
      "2  2/6/2022    KY     40288  current  current  current       elgiible   \n",
      "3  2/6/2022    KY     40288  current  current  current       elgiible   \n",
      "4  2/6/2022    KY     40288  current  current  current       elgiible   \n",
      "\n",
      "         roll_30        roll_60        roll_90 segment  bucket  \n",
      "0   roll forward   roll forward   roll forward    cure       1  \n",
      "1  roll backward  roll backward  roll backward    cure       1  \n",
      "2  roll backward  roll backward  roll backward    cure       1  \n",
      "3  roll backward  roll backward  roll backward    cure       1  \n",
      "4   roll forward  roll backward  roll backward    cure       1  \n",
      "(259, 24)\n",
      "Index(['acct_ref_nb', 'date', 'month', 'EMAIL', 'OUTBOUND', 'TEXT',\n",
      "       'target_channel', 'acct_balance', 'tot_delq_amt', 'min_delq_amt',\n",
      "       'probability_score', 'src_data_dt', 'cycle_dt', 'state', 'zip_code',\n",
      "       'dlq_30', 'dlq_60', 'dlq_90', 'exclusion_type', 'roll_30', 'roll_60',\n",
      "       'roll_90', 'segment', 'bucket'],\n",
      "      dtype='object')\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: bucket, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#print(jan_dlq_no_dupes)\n",
    "#print(jan_dlq_no_dupes.shape)\n",
    "\n",
    "# Merge data on acct_ref_nb and month\n",
    "data = pd.merge(df_agg_concatenate, jan_dlq_no_dupes, on=[\"acct_ref_nb\", \"month\"], how=\"left\") #events_df_agg and jan_dlq_no_dupes for when this is implemented, check the data names for the ROC at the end\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "#print datas columns\n",
    "print(data.columns)\n",
    "#how to select just the buckets column to show\n",
    "print(data['bucket'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   acct_ref_nb        date      month  EMAIL  OUTBOUND  TEXT target_channel  \\\n",
      "0       125001  2022-01-31 2022-01-01      1         1     1           PYMT   \n",
      "1       125001  2022-02-01 2022-02-01      1         1     1           PYMT   \n",
      "2       125001  2022-02-01 2022-02-01      1         1     1           PYMT   \n",
      "3       125001  2022-02-01 2022-02-01      1         1     1           PYMT   \n",
      "4       125001  2022-02-01 2022-02-01      1         1     1           PYMT   \n",
      "\n",
      "   acct_balance  tot_delq_amt  min_delq_amt  probability_score src_data_dt  \\\n",
      "0           250           100            25              0.534  2022-01-31   \n",
      "1           500           250            50              0.734  2022-02-01   \n",
      "2           500           250            50              0.434  2022-02-02   \n",
      "3           500           250            50              0.334  2022-02-02   \n",
      "4           500           250            50              0.120  2022-02-03   \n",
      "\n",
      "   cycle_dt state  zip_code   dlq_30   dlq_60   dlq_90 exclusion_type  \\\n",
      "0  1/5/2022    KY     40288  current  current  current       elgiible   \n",
      "1  2/6/2022    KY     40288  current  current  current       elgiible   \n",
      "2  2/6/2022    KY     40288  current  current  current       elgiible   \n",
      "3  2/6/2022    KY     40288  current  current  current       elgiible   \n",
      "4  2/6/2022    KY     40288  current  current  current       elgiible   \n",
      "\n",
      "         roll_30        roll_60        roll_90 segment  bucket  \\\n",
      "0   roll forward   roll forward   roll forward    cure       1   \n",
      "1  roll backward  roll backward  roll backward    cure       1   \n",
      "2  roll backward  roll backward  roll backward    cure       1   \n",
      "3  roll backward  roll backward  roll backward    cure       1   \n",
      "4   roll forward  roll backward  roll backward    cure       1   \n",
      "\n",
      "  long_term_risk_score acct_balance_segments tot_delq_balance_segments  \\\n",
      "0              0.4-0.6               100-500                   100-500   \n",
      "1              0.6-0.8              500-1000                   100-500   \n",
      "2              0.4-0.6              500-1000                   100-500   \n",
      "3              0.2-0.4              500-1000                   100-500   \n",
      "4                0-0.2              500-1000                   100-500   \n",
      "\n",
      "  min_delq_balance_segments  \n",
      "0                     0-100  \n",
      "1                     0-100  \n",
      "2                     0-100  \n",
      "3                     0-100  \n",
      "4                     0-100  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define bucket boundaries and labels for different features\n",
    "bucket_definitions = {\n",
    "    'probability_score': {\n",
    "        'bins': [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        'labels': ['0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0'],\n",
    "        'column_name': 'long_term_risk_score'\n",
    "    },\n",
    "    'acct_balance': {\n",
    "        'bins': [0, 100, 500, 1000, 5000, 10000, 50000, 100000],\n",
    "        'labels': ['0-100', '100-500', '500-1000', '1000-5000', '5000-10000', '10000-50000', '50000-100000'],  # Added one label\n",
    "        'column_name': 'acct_balance_segments'\n",
    "    },\n",
    "    'tot_delq_amt': {\n",
    "        'bins': [0, 100, 500, 1000, 5000, 10000, 50000, 100000],\n",
    "        'labels': ['0-100', '100-500', '500-1000', '1000-5000', '5000-10000', '10000-50000', '50000-100000'],  # Added one label\n",
    "        'column_name': 'tot_delq_balance_segments'\n",
    "    },\n",
    "    'min_delq_amt': {\n",
    "        'bins': [0, 100, 500, 1000, 5000, 10000, 50000, 100000],\n",
    "        'labels': ['0-100', '100-500', '500-1000', '1000-5000', '5000-10000', '10000-50000', '50000-100000'],  # Added one label\n",
    "        'column_name': 'min_delq_balance_segments'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Cut data into buckets\n",
    "for feature, definitions in bucket_definitions.items():\n",
    "    column_name = definitions['column_name']\n",
    "    data[column_name] = pd.cut(data[feature], bins=definitions['bins'], labels=definitions['labels'], include_lowest=True, right=False)\n",
    "print(data.head())\n",
    "# export pre-preprocessor data to csv\n",
    "#data.to_csv(r\"C:\\Users\\ghadf\\OneDrive\\Desktop\\Data Analytics\\Python\\DL\\channel_engage\\data\\prepreprocessed_data.csv\", index=False)\n",
    "\n",
    "# If you're going to use models like linear or logistic regression, use the following lines to one-hot encode\n",
    "#columns_to_onehot = [defi['column_name'] for defi in bucket_definitions.values()]\n",
    "#data = pd.get_dummies(data, columns=columns_to_onehot, prefix='', prefix_sep='')\n",
    "\n",
    "# Change date to year-month-day\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "\n",
    "#print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(259, 30)\n",
      "target_channel       NO_PYMT  PYMT\n",
      "interaction_channel               \n",
      "EMAIL                    176    83\n",
      "OUTBOUND                 128    51\n",
      "TEXT                     128    35\n"
     ]
    }
   ],
   "source": [
    "#check for nulls step: will say if there are nulls in the data\n",
    "if data.isnull().values.any():\n",
    "    print(\"There are nulls in the data\")\n",
    "#print(data.isnull().sum())\n",
    "\n",
    "#drop nulls\n",
    "data = data.dropna()\n",
    "print(data.shape)\n",
    "\n",
    "#view the dataframe by interaction channels to understand the weight of each channel\n",
    "\n",
    "# Reshape the dataframe using melt to have interaction channels in one column\n",
    "melted_data = pd.melt(data, id_vars=['target_channel'], value_vars=interaction_channels, var_name='interaction_channel', value_name='value')\n",
    "\n",
    "# Filter only rows where value is 1 (i.e., the interaction happened)\n",
    "filtered_melted_data = melted_data[melted_data['value'] == 1]\n",
    "\n",
    "# Create the contingency table\n",
    "contingency_table = pd.crosstab(filtered_melted_data['interaction_channel'], filtered_melted_data['target_channel'])\n",
    "\n",
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   month  EMAIL  OUTBOUND  TEXT target_channel   dlq_30   dlq_60   dlq_90  \\\n",
      "0      1      1         1     1           PYMT  current  current  current   \n",
      "1      2      1         1     1           PYMT  current  current  current   \n",
      "2      2      1         1     1           PYMT  current  current  current   \n",
      "3      2      1         1     1           PYMT  current  current  current   \n",
      "4      2      1         1     1           PYMT  current  current  current   \n",
      "\n",
      "  exclusion_type        roll_30        roll_60        roll_90 segment  bucket  \\\n",
      "0       elgiible   roll forward   roll forward   roll forward    cure       1   \n",
      "1       elgiible  roll backward  roll backward  roll backward    cure       1   \n",
      "2       elgiible  roll backward  roll backward  roll backward    cure       1   \n",
      "3       elgiible  roll backward  roll backward  roll backward    cure       1   \n",
      "4       elgiible   roll forward  roll backward  roll backward    cure       1   \n",
      "\n",
      "  long_term_risk_score acct_balance_segments tot_delq_balance_segments  \\\n",
      "0              0.4-0.6               100-500                   100-500   \n",
      "1              0.6-0.8              500-1000                   100-500   \n",
      "2              0.4-0.6              500-1000                   100-500   \n",
      "3              0.2-0.4              500-1000                   100-500   \n",
      "4                0-0.2              500-1000                   100-500   \n",
      "\n",
      "  min_delq_balance_segments  year  day  \n",
      "0                     0-100  2022   31  \n",
      "1                     0-100  2022    1  \n",
      "2                     0-100  2022    1  \n",
      "3                     0-100  2022    1  \n",
      "4                     0-100  2022    1  \n",
      "(259, 20)\n",
      "   month  EMAIL  OUTBOUND  TEXT   dlq_30   dlq_60   dlq_90 exclusion_type  \\\n",
      "0      1      1         1     1  current  current  current       elgiible   \n",
      "1      2      1         1     1  current  current  current       elgiible   \n",
      "2      2      1         1     1  current  current  current       elgiible   \n",
      "3      2      1         1     1  current  current  current       elgiible   \n",
      "4      2      1         1     1  current  current  current       elgiible   \n",
      "\n",
      "         roll_30        roll_60        roll_90 segment  bucket  \\\n",
      "0   roll forward   roll forward   roll forward    cure       1   \n",
      "1  roll backward  roll backward  roll backward    cure       1   \n",
      "2  roll backward  roll backward  roll backward    cure       1   \n",
      "3  roll backward  roll backward  roll backward    cure       1   \n",
      "4   roll forward  roll backward  roll backward    cure       1   \n",
      "\n",
      "  long_term_risk_score acct_balance_segments tot_delq_balance_segments  \\\n",
      "0              0.4-0.6               100-500                   100-500   \n",
      "1              0.6-0.8              500-1000                   100-500   \n",
      "2              0.4-0.6              500-1000                   100-500   \n",
      "3              0.2-0.4              500-1000                   100-500   \n",
      "4                0-0.2              500-1000                   100-500   \n",
      "\n",
      "  min_delq_balance_segments  year  day  \n",
      "0                     0-100  2022   31  \n",
      "1                     0-100  2022    1  \n",
      "2                     0-100  2022    1  \n",
      "3                     0-100  2022    1  \n",
      "4                     0-100  2022    1  \n"
     ]
    }
   ],
   "source": [
    "#Preprocessing \n",
    "# Drop any non-essential columns\n",
    "data = data.drop(columns=['cycle_dt', 'src_data_dt', 'date','acct_ref_nb', 'zip_code', 'state',\n",
    "                            'acct_balance', 'tot_delq_amt', 'min_delq_amt', 'probability_score']) #account number is not needed because identifier is src_data_dt\n",
    "\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "# export pre-preprocessor data to csv\n",
    "data.to_csv(r\"C:\\Users\\ghadf\\OneDrive\\Desktop\\Data Analytics\\Python\\DL\\channel_engage\\data\\preprocessed_data.csv\", index=False)\n",
    "\n",
    "# Define feature lists\n",
    "numerical_features = [ 'year', 'month', 'day', 'EMAIL', 'TEXT', 'OUTBOUND']\n",
    "categorical_features = [col for col in data.columns if col not in numerical_features + [ 'target_channel']] #'interaction_channel' is needed in here while target_channel is the y variable\n",
    "\n",
    "# Define transformers\n",
    "numerical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_transformer, numerical_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Define X and y\n",
    "X = data.drop(columns=['target_channel'])\n",
    "print(X.head())\n",
    "# Use LabelEncoder to encode target_channel\n",
    "label_encoder = LabelEncoder()\n",
    "y = data['target_channel']\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "#print(y_encoded)\n",
    "\n",
    "# Preprocess the data\n",
    "X = preprocessor.fit_transform(X)\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([155, 30]) torch.Size([155]) torch.Size([52, 30]) torch.Size([52]) torch.Size([52, 30]) torch.Size([52])\n",
      "NO_PYMT    176\n",
      "PYMT        83\n",
      "Name: target_channel, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Neural Network Model: Splitting the data into train, validation, and test sets\n",
    "# If X is a sparse matrix, convert to a dense matrix\n",
    "if hasattr(X, \"todense\"):\n",
    "    X = X.todense()\n",
    "\n",
    "X = np.asarray(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_temp, y_train_encoded, y_temp_encoded = train_test_split(X, y_encoded, test_size=0.4, stratify=y_encoded) # can add stratify=y_encoded to make sure the split is even\n",
    "X_val, X_test, y_val_encoded, y_test_encoded = train_test_split(X_temp, y_temp_encoded, test_size=0.5, stratify=y_temp_encoded) # can add stratify=y_encoded to make sure the split is even\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train_encoded)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.LongTensor(y_val_encoded)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test_encoded)\n",
    "\n",
    "# Convert to PyTorch datasets and create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "print(X_train_tensor.shape, y_train_tensor.shape, X_val_tensor.shape, y_val_tensor.shape, X_test_tensor.shape, y_test_tensor.shape)\n",
    "print(data['target_channel'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
